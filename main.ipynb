{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Let's get it started with the XTREME benchmark from Hugging Face datasets.\n",
    "To import the dataset, we can use the `load_dataset` function from the `datasets` library.\n",
    "This benchmark includes a variety of tasks across multiple languages, making it a great choice for evaluating multilingual models.\n",
    "It use IOB format for sequence labeling tasks, which is a common format for named entity recognition (NER) and other similar tasks."
   ],
   "id": "d9f6f63db2118abe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:07:21.005678Z",
     "start_time": "2025-11-11T15:07:11.730258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")\n"
   ],
   "id": "3ecad7506c0bc486",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Whoa, that‚Äôs a lot of configurations! `XTREME` includes a variety of tasks such as:\n",
    "- Named Entity Recognition (NER)\n",
    "- Part-of-Speech Tagging (POS)\n",
    "- Question Answering (QA)\n",
    "- Sentence Retrieval (SR)\n",
    "\n",
    "But we'll focus on the `NER` task for this example.\n",
    "Let‚Äôs narrow the search by just looking for the configurations that start with ‚Äú`PAN`‚Äù\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Because `PAN-X` is the subset of `XTREME` that focuses on `NER` across multiple languages."
   ],
   "id": "1cc29c284907f92a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:07:21.346654Z",
     "start_time": "2025-11-11T15:07:21.334299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ],
   "id": "31c25c51d03352f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, we have several configurations for `PAN-X`, each corresponding to a different language.\n",
    "Like you can see, each one has a two-letter language code at the end, such as `en` for English, `de` for German, and `fr` for French. it follows the **ISO 639-1** standard for language codes.\n",
    "\n",
    "Alright, if we want to use the German corpus, we can load it like this:"
   ],
   "id": "d2c2926cbad5bcc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:07:32.882699Z",
     "start_time": "2025-11-11T15:07:21.563264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\")"
   ],
   "id": "1bd4d271b822037b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But what if we want to load multiple languages at once? for exemple, Swiss corpus which includes German, French, English and Italian.\n",
    "\n",
    "This corpus is particularly interesting because it reflects the multilingual nature of Switzerland, where multiple languages are spoken and imbalanced.\n",
    "\n",
    "We have like:\n",
    "- 62% of German (de)\n",
    "- 22% of French (fr)\n",
    "- 8% of Italian (it)\n",
    "- 5% of English (en)\n",
    "\n",
    "So, To keep track of each language, let‚Äôs create a Python `defaultdict` that stores the language code as the `key` and a `PAN-X` corpus of type DatasetDict as the value:"
   ],
   "id": "9109f04cb0e19611"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:19.119869Z",
     "start_time": "2025-11-11T15:07:32.905194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "# Return a DatasetDict if a key doesn't exist\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # Shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))\n"
   ],
   "id": "174b4ff5ae192d1d",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To ensure that our dataset don't accidentally bias our dataset splits, we `shuffle` each split with a fixed seed before downsampling it according to the spoken proportion.\n",
    "\n",
    "Let's take a look at the number of training examples in each language:"
   ],
   "id": "c99aa2f1ea619c81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:19.396393Z",
     "start_time": "2025-11-11T15:08:19.383190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    "             index=[\"Number of training examples\"])"
   ],
   "id": "4ca23f2863dfe17c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, we have more training examples for Geman than for the other languages, which reflects the linguistic landscape of Switzerland.\n",
    "\n",
    "So, we can use it as a starting point from which zero-shot cross-lingual transfer\n",
    "to French, Italian, and English.\n",
    "\n",
    "Let's take a look at a few examples from the German training set:"
   ],
   "id": "d5941579059d012b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:19.697198Z",
     "start_time": "2025-11-11T15:08:19.664145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ],
   "id": "5b89c4faacdac224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, each example consists of a sentence and its corresponding named entity tags in IOB format.\n",
    "\n",
    "ner_tags column corresponds to the mapping of each entity to a class ID. This is a bit cryptic, so let's add a column that maps each class ID to its corresponding entity label\n",
    "\n",
    "First, let's take a look at the features of the dataset to find the mapping:"
   ],
   "id": "51bfb586ad367d84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:20.163582Z",
     "start_time": "2025-11-11T15:08:20.150961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "id": "981dfe4c8cce52c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: List(Value('string'))\n",
      "ner_tags: List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']))\n",
      "langs: List(Value('string'))\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ner_tags` feature is of type `ClassLabel`, which means it has a predefined set of labels.\n",
    "\n",
    "Let's pick up the mapping from class IDs to entity labels:"
   ],
   "id": "995d81128355df8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:20.492640Z",
     "start_time": "2025-11-11T15:08:20.484759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ],
   "id": "d5541411e897bc27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ClassLabel` object provides a method called `int2str` that allows us to convert class IDs to their corresponding string labels.\n",
    "With `map` method, we can easily create a new column in the dataset that contains the string labels for each entity tag."
   ],
   "id": "4f93aea13193b04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:20.712429Z",
     "start_time": "2025-11-11T15:08:20.659331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "\n",
    "\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ],
   "id": "43fe789d87f97b78",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now, let's take a look at the first example in the German training set with the new `ner_tags_str` column\n",
    "> Yeah, this is a data Analyst Habits! üòÖ"
   ],
   "id": "9b5516e427e4494c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:20.843546Z",
     "start_time": "2025-11-11T15:08:20.826247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "             ['Tokens', 'Tags'])\n"
   ],
   "id": "1b81a9b5b449478f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The presence of the `LOC` tags make sense since the sentence ‚Äú2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern‚Äù means ‚Äú2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania‚Äù in English. And ‚Äú**Danziger Bucht**‚Äù is indeed a location, a bay in the Baltic sea.\n",
   "id": "edbc278c544ccd14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's make a quick check to see if we don't have any unusual imbalance in the tags, let's look at the distribution of each entity across each split.",
   "id": "f16ae714f1b883f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:22.544098Z",
     "start_time": "2025-11-11T15:08:21.094994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ],
   "id": "89a30b3c47fbf291",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a pretty good distribution of entity tags across the `training`, `validation`, and `test` sets.\n",
    "\n",
    "`LOC`, `PER`, and `ORG` are roughly the same for each split, which is what we want to see.\n"
   ],
   "id": "6b17b811b9d82a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# What are we going to do next?\n",
    "\n",
    "Like i mentioned earlier, we are going to make a zero-shot cross-lingual transfer from German to French, Italian, and English.\n",
    "\n",
    "What is Zero-shot cross-lingual transfer?\n",
    "\n",
    ">In short, it means training a model on one language (German in this case) and then evaluating its performance on other languages (French, Italian, and English) without any additional training on those languages."
   ],
   "id": "89ea20fcb1a0e843"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, now, we need a model to evaluate.\n",
    "\n",
    "One of the first multilingual transformers was `mBERT`, which uses the same architecture and pretraining objective as `BERT` but is trained on additional data from wikipedia articles in many languages. After that, there are many other models like `XLM-R`, `mT5`, and `mDeBERTaV3` that have shown even better performance on various multilingual benchmarks.\n",
    "\n",
    "So we will focus on `XLM-R`, which is a robustly optimized version of `mBERT` and has shown strong performance on various multilingual benchmarks, including `XTREME`."
   ],
   "id": "197048d383bdb54a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`XLM-R` is a transformer-based model like `BERT`, but use a `SentencePiece` tokenizer instead of `WordPiece` tokenizer used in `BERT`.\n",
    "\n",
    "To get a feel for how SentencePiece compares to WordPiece, let's load the BERT tokenizer and the XLM-R tokenizer in ü§ó(Hugging Face) Transformers."
   ],
   "id": "795585b3e0a74d75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:26.246100Z",
     "start_time": "2025-11-11T15:08:22.810903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ],
   "id": "80c9aced436353d0",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now, let's take a example sentence",
   "id": "70b353a36125d3dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:26.676014Z",
     "start_time": "2025-11-11T15:08:26.663501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "bert_tokens, xlmr_tokens"
   ],
   "id": "3611a00d3c524dc9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]'],\n",
       " ['<s>', '‚ñÅJack', '‚ñÅSpar', 'row', '‚ñÅlove', 's', '‚ñÅNew', '‚ñÅYork', '!', '</s>'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, one of the main differences is that instead of the `[CLS]` and `[SEP]` tokens that `BERT` uses for sentence\n",
    "classification tasks, `XLM-R` uses `<s>` and `<\\s>` to denote the start and end of a sequence\n",
    "\n",
    "One other difference is how the two tokenizers handle subword tokenization. `BERT` uses `##` to indicate that a token is a continuation of the previous token, while `XLM-R` uses `‚ñÅ` to indicate the start of a new word or just a space.\n",
    "\n",
    "Here, `BERT` tokenizer lost the information that there is no whitespace between ‚ÄúYork‚Äù and ‚Äú!‚Äù\n",
    "\n",
    "Why?\n",
    "\n",
    ">In short, the `WordPiece` tokenizer first splits text on whitespace and punctuation, then breaks each word into subword units from its vocabulary. It does not explicitly preserve whitespace information.\n",
    "In contrast, the `SentencePiece` tokenizer treats the input as a raw character sequence and uses a statistical model (`Unigram` or `BPE`) to segment it, encoding spaces explicitly and requiring no pre-tokenization."
   ],
   "id": "8e5ba305d0139308"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:27.020480Z",
     "start_time": "2025-11-11T15:08:27.012449Z"
    }
   },
   "cell_type": "code",
   "source": "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")",
   "id": "6c17abb1e59eaeba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Do you know how is the transformer model architecture looks like?\n",
    "in general, you have two main components:\n",
    "1. The model body (the transformer layers)\n",
    "2. The task-specific head (like a classification head for text classification tasks or a token classification head for NER tasks)\n",
    "\n",
    "so, take it like a sample exercise, let's implement a token classification model using `XLM-R` as the model body.\n",
    "\n",
    "> Note: In fact, ü§ó Transformers library already provides a pre-implemented class called `XLMRobertaForTokenClassification` that does exactly this. But implementing it from scratch is a great way to understand how these models work under the hood. And if someday you need a custom modification, you will know where to start.\n"
   ],
   "id": "a047de5eff80d23c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:27.259894Z",
     "start_time": "2025-11-11T15:08:27.243765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)\n"
   ],
   "id": "3f5b73848caa4ee5",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, we first load the `XLM-R` model body using the `RobertaModel` class from ü§ó Transformers. Then, we set up a token classification head consisting of a dropout layer followed by a linear layer that maps the hidden states to the number of labels.\n",
    "\n",
    "My custom class inherits from `RobertaPreTrainedModel`, which provides useful methods for loading and saving pretrained models.\n",
    "\n",
    "But, you might wondering, what does do `config_class` ?\n",
    "> In ü§ó transformers, each model inherits from a base class like `PreTrainedModel` (here `RobertaPreTrainedModel`).\n",
    "> These base classes define several utility methods such as:\n",
    "> - rom_pretrained(...)\n",
    "> - save_pretrained(...)\n",
    "> - from_config(...)\n",
    ">\n",
    "> These methods often need to know which configuration class to use for the specific model.\n",
    "> By setting the `config_class` attribute, we inform the base class about the appropriate configuration class to use when instantiating the model from a configuration object.\n",
    "\n",
    "Note that, we set `add_pooling_layer=False` to ensure all `hidden states` are returned and not only the one\n",
    "associated with the `[CLS]` token"
   ],
   "id": "c799dde8df80beb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we are ready to load our token classification model. However, first, We‚Äôll need to provide some\n",
    "additional information beyond the model name, including the tags that we will use to\n",
    "label each entity and the mapping of each tag to an ID and vice versa"
   ],
   "id": "5af42a757887fb05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:27.461254Z",
     "start_time": "2025-11-11T15:08:27.445403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ],
   "id": "c0f14d290504324c",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll store thes mapping and the `tags.num_classes` attribute in the AutoConfig object by passing them as keyword arguments to the `from_pretrained` method.",
   "id": "ac8de1696c040c62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:27.971096Z",
     "start_time": "2025-11-11T15:08:27.640967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ],
   "id": "ce977b2a5732dd0d",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can load our token classification model using the `from_pretrained` method of our custom class and passing the configuration object we just created.",
   "id": "75c894703e561f43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:32.057894Z",
     "start_time": "2025-11-11T15:08:28.093557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ],
   "id": "571c977427d4605e",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's make a quick check to see if the model is working as expected",
   "id": "c6013765844e8640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:32.200468Z",
     "start_time": "2025-11-11T15:08:32.185762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ],
   "id": "557ebcaf810dd51d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ‚ñÅJack  ‚ñÅSpar    row  ‚ñÅlove  s  ‚ñÅNew  ‚ñÅYork   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅJack</td>\n",
       "      <td>‚ñÅSpar</td>\n",
       "      <td>row</td>\n",
       "      <td>‚ñÅlove</td>\n",
       "      <td>s</td>\n",
       "      <td>‚ñÅNew</td>\n",
       "      <td>‚ñÅYork</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we pass the input IDs to the model and extract the predicted class for each token by taking the `argmax` of the output logits.",
   "id": "c8aed3391ab6db46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:32.428301Z",
     "start_time": "2025-11-11T15:08:32.403204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ],
   "id": "cfdf0d2d4c869e51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n",
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, the logits have the shape `[batch_size, num_tokens, num_tags]`, with each token having a score for each seven possible tags.\n",
    "\n",
    "By enumerating over the sequence, we can quickly see the predicted tag for each token."
   ],
   "id": "3f5406372e38af43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:32.967838Z",
     "start_time": "2025-11-11T15:08:32.934764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ],
   "id": "9f97152c12e13b80",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ‚ñÅJack  ‚ñÅSpar    row  ‚ñÅlove      s   ‚ñÅNew  ‚ñÅYork      !   </s>\n",
       "Tags    I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  B-PER"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅJack</td>\n",
       "      <td>‚ñÅSpar</td>\n",
       "      <td>row</td>\n",
       "      <td>‚ñÅlove</td>\n",
       "      <td>s</td>\n",
       "      <td>‚ñÅNew</td>\n",
       "      <td>‚ñÅYork</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So... I don't think \"Jack\" is a Location, right? üòÖ\n",
    "\n",
    "But well, with random weights ‚Äî what did I expect anyway üòÖ\n",
    "\n",
    "Okay, time to fine-tune it on some labeled data ‚Äî let‚Äôs make it smarter üòé\n",
    "Before that, though, let‚Äôs wrap the previous steps into a neat helper function."
   ],
   "id": "8067dc218b95f80a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:33.348197Z",
     "start_time": "2025-11-11T15:08:33.341946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ],
   "id": "1c998735632ca37b",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before we start to train our model, we need to tokenize the input and prepare the labels.\n",
    "\n",
    "So, like we can see, the tokenizer and model can encode a single example. Our next step is to tokenize the entire dataset so that we can feed it into the model for fine-tuning.\n",
    "\n",
    "ü§ó Datasets provide a convenient `map` method that allows us to apply a function to each example in the dataset.\n",
    "\n",
    "For that, we need to define a function with the minimum signature :\n",
    "\n",
    "`function(examples: Dict[str, List]) -> Dict[str, List]`\n",
    "\n",
    "Since the XLM-R tokenizer returns the input IDs, but we also need :\n",
    "- first the attention masks to indicate which tokens are real tokens and which are padding tokens.\n",
    "- second, the label IDs to say which tag (e.g., `B-PER`, `I-LOC`, etc.) corresponds to each token.\n",
    "\n",
    "Let's look at how this works for a single example:"
   ],
   "id": "9a5ff5e838afb584"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:33.936881Z",
     "start_time": "2025-11-11T15:08:33.902664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n",
    "words, labels"
   ],
   "id": "ff610a0061d3dd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2.000',\n",
       "  'Einwohnern',\n",
       "  'an',\n",
       "  'der',\n",
       "  'Danziger',\n",
       "  'Bucht',\n",
       "  'in',\n",
       "  'der',\n",
       "  'polnischen',\n",
       "  'Woiwodschaft',\n",
       "  'Pommern',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we can use the tokenizer to tokenize the input words. Since our input is already tokenized into words, we need to set the `is_split_into_words` parameter to `True` to say to the tokenizer that the input is already split into words.",
   "id": "13ceb7fc7625497f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:34.332723Z",
     "start_time": "2025-11-11T15:08:34.289738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ],
   "id": "998dc09d580e7f08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         0       1           2  3    4     5     6   7    8      9   ...   15  \\\n",
       "Tokens  <s>  ‚ñÅ2.000  ‚ñÅEinwohner  n  ‚ñÅan  ‚ñÅder  ‚ñÅDan  zi  ger  ‚ñÅBuch  ...  ‚ñÅWo   \n",
       "\n",
       "       16   17      18   19    20 21 22 23    24  \n",
       "Tokens  i  wod  schaft  ‚ñÅPo  mmer  n  ‚ñÅ  .  </s>  \n",
       "\n",
       "[1 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅ2.000</td>\n",
       "      <td>‚ñÅEinwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅan</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅDan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>‚ñÅBuch</td>\n",
       "      <td>...</td>\n",
       "      <td>‚ñÅWo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>‚ñÅPo</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like we can see, the tokenizer has split \"_Einwohnern\" into two subwords: \"‚ñÅEinwohner\" and \"n\".\n",
    "\n",
    "Since we're following the convention that \"_Einwohner\" should be associated with the \"B-LOC\", we need a way to mask the subword representations after the first one. Fortunately, the tokenizer provides a method called `word_ids()` that returns a list mapping each token to its corresponding word index in the original input."
   ],
   "id": "46b8b1c4dd726ae7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:34.702782Z",
     "start_time": "2025-11-11T15:08:34.670568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ],
   "id": "d84c7e8af0b96057",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0       1           2  3    4     5     6   7    8      9   ...  \\\n",
       "Tokens     <s>  ‚ñÅ2.000  ‚ñÅEinwohner  n  ‚ñÅan  ‚ñÅder  ‚ñÅDan  zi  ger  ‚ñÅBuch  ...   \n",
       "Word IDs  None       0           1  1    2     3     4   4    4      5  ...   \n",
       "\n",
       "           15 16   17      18   19    20  21  22  23    24  \n",
       "Tokens    ‚ñÅWo  i  wod  schaft  ‚ñÅPo  mmer   n   ‚ñÅ   .  </s>  \n",
       "Word IDs    9  9    9       9   10    10  10  11  11  None  \n",
       "\n",
       "[2 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅ2.000</td>\n",
       "      <td>‚ñÅEinwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅan</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅDan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>‚ñÅBuch</td>\n",
       "      <td>...</td>\n",
       "      <td>‚ñÅWo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>‚ñÅPo</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, each token is associated with a word index, where `None` indicates special tokens like `<s>` and `<\\s>`.\n",
    "The original words are indexed from `0` to `n-1`, where `n` is the number of words in the input.\n",
    "\n",
    "So... we can use this mapping to align the original labels with the tokenized input and now we know if for exemple \"n\" is a continuation of the previous word or not.\n",
    "\n",
    "Let's set `-100` for the tokens that we want to ignore during loss computation."
   ],
   "id": "2ed11240d11717ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:35.032951Z",
     "start_time": "2025-11-11T15:08:35.009262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ],
   "id": "5583b4b2372ffad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             0       1           2     3    4     5      6     7     8   \\\n",
       "Tokens      <s>  ‚ñÅ2.000  ‚ñÅEinwohner     n  ‚ñÅan  ‚ñÅder   ‚ñÅDan    zi   ger   \n",
       "Word IDs   None       0           1     1    2     3      4     4     4   \n",
       "Label IDs  -100       0           0  -100    0     0      5  -100  -100   \n",
       "Labels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n",
       "\n",
       "              9   ...     15    16    17      18     19    20    21  22    23  \\\n",
       "Tokens     ‚ñÅBuch  ...    ‚ñÅWo     i   wod  schaft    ‚ñÅPo  mmer     n   ‚ñÅ     .   \n",
       "Word IDs       5  ...      9     9     9       9     10    10    10  11    11   \n",
       "Label IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \n",
       "Labels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n",
       "\n",
       "             24  \n",
       "Tokens     </s>  \n",
       "Word IDs   None  \n",
       "Label IDs  -100  \n",
       "Labels      IGN  \n",
       "\n",
       "[4 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅ2.000</td>\n",
       "      <td>‚ñÅEinwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅan</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅDan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>‚ñÅBuch</td>\n",
       "      <td>...</td>\n",
       "      <td>‚ñÅWo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>‚ñÅPo</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>...</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Maybe you might wonder, **why ignore the special tokens and subword tokens during loss computation?**\n",
    "\n",
    "> Ignoring special tokens and subword tokens during loss computation is important because these tokens do not correspond to actual words in the input text. Including them in the loss calculation could introduce noise and lead to incorrect learning signals for the model.\n",
    "\n",
    "And second, **why set the label of subword tokens to -100 specifically?**\n",
    "\n",
    "> In PyTorch's `CrossEntropyLoss`, the label `-100` is used as a special value to indicate that a particular token should be ignored during loss computation. This is a convention in PyTorch, and using `-100` allows us to effectively exclude those tokens from contributing to the loss, ensuring that the model focuses on learning from the relevant tokens only.\n",
    "\n",
    "Now, we can wrap all these steps into a single function that we can use with the `map` method of the dataset."
   ],
   "id": "b6f2a3211d82f0d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:36.059192Z",
     "start_time": "2025-11-11T15:08:36.053149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ],
   "id": "bcd19b0bf439b527",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can write a function to iterate over.",
   "id": "87890f0237b8ac8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:36.747537Z",
     "start_time": "2025-11-11T15:08:36.740628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])"
   ],
   "id": "5795885fc92e7aa3",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When we use it to a DatasetDict, we'll get a encoded DatasetDict per split.",
   "id": "cb212693e204465b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:37.396134Z",
     "start_time": "2025-11-11T15:08:37.166332Z"
    }
   },
   "cell_type": "code",
   "source": "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])",
   "id": "d6455591ebdde01f",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we have the model and the encoded dataset, we need to define a performance metric to evaluate the model during training and validation.\n",
    "\n",
    "To evaluate NER models, we use metrics like `precision`, `recall`, and `F1-score`, which are commonly used in classification tasks. But in NER, a prediction is considered correct only if all parts of the entity are correctly identified and classified.\n",
    "\n",
    "For example, if the true entity is `New York` labeled as `B-LOC` and `I-LOC`, but the model predicts \"New\" as `B-LOC` and \"York\" as `O` (outside), this would be considered an incorrect prediction.\n",
    "\n",
    "Fortunately, the `seqeval` library provides a convenient way to compute these metrics specifically for sequence labeling tasks like NER. So... let's try it out."
   ],
   "id": "859bc2ff0f76bb49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:37.599636Z",
     "start_time": "2025-11-11T15:08:37.584134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ],
   "id": "a9b2d7cabf5f228f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, `seqeval` expects the predictions and labels as lists of lists. with each list corresponding to a sequence of tags for a single example in our validation or test sets.\n",
    "\n",
    "So, to use it during training, we need to write a function that takes the model outputs and the true labels, converts them onto lists that `seqeval` can understand, and then computes the metrics. we also need to make sure to ignore the tokens with label `-100` during this process because they were not part of the original input."
   ],
   "id": "2432aefa78ae40f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:37.783752Z",
     "start_time": "2025-11-11T15:08:37.775921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "    return preds_list, labels_list"
   ],
   "id": "8fca021f2963cf2c",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we can move on to the training part.\n",
    "\n",
    "Our first strategy is to fine-tune the `XLM-R` model on the German `PAN-X` corpus and then evaluate its performance on the French, Italian, and English `PAN-X` corpora. Obviously, we use ü§ó Transformers `Trainer` class to handle the training loop, evaluation, and other details for us."
   ],
   "id": "9e67ee2d280f25df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:38.045311Z",
     "start_time": "2025-11-11T15:08:37.943703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, eval_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=True)\n"
   ],
   "id": "8317dd07c0fbdad4",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It also a good point to make sure that our model is able to push the fine-tuned weights to the Hugging Face Hub after training.",
   "id": "a1cca13c47f23e85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:38.235620Z",
     "start_time": "2025-11-11T15:08:38.179436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "id": "33dbdea5711bf15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40a4149b17424adeab0b9962d79e1643"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also need to tell the Trainer how to compute the evaluation metrics during training and validation. so here, we can use the `align_predictions` function we defined earlier to convert the model outputs and true labels into the format expected by `seqeval`.",
   "id": "d674e68b5844ffa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:38.415125Z",
     "start_time": "2025-11-11T15:08:38.410663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}\n"
   ],
   "id": "961d88c0467b230a",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And the final step is to define a data collator to pad the input sequences to the same length within a batch. ü§ó Transformers provides a convenient `DataCollatorForTokenClassification` class that does exactly this for token classification tasks.",
   "id": "387d3a1ecc868d9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:38.592379Z",
     "start_time": "2025-11-11T15:08:38.585976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ],
   "id": "60befa4f5badaa81",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can define a function that initializes the model. This is useful when using multiple GPUs or TPUs, as it ensures that each device gets its own copy of the model.",
   "id": "e0ff9ede18ea31cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:38.764499Z",
     "start_time": "2025-11-11T15:08:38.759162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))"
   ],
   "id": "6be7788eeecd771e",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now pass all this information together with the encoded datasets to the Trainer",
   "id": "ac0a7df5045f7a4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:40.608751Z",
     "start_time": "2025-11-11T15:08:38.926435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=panx_de_encoded[\"train\"],\n",
    "                  eval_dataset=panx_de_encoded[\"validation\"],\n",
    "                  processing_class=xlmr_tokenizer)\n"
   ],
   "id": "df0f959601eae1ce",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now, we just need to call the `train` method to start fine-tuning the model on the German `PAN-X` corpus.\n",
    "\n",
    "In my case, I made my training on Google Colab because of the limited computational resources. After training, we can push the fine-tuned model to the Hugging Face Hub using the `push_to_hub` method.\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")\n",
    "```\n"
   ],
   "id": "d94287d3bc1c69a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You might have something like this after training:\n",
    "\n",
    "![](image/img.png)\n",
    "\n",
    "And now, you can load the fine-tuned model directly from the Hugging Face Hub using the `from_pretrained` method of the `AutoModelForTokenClassification` class."
   ],
   "id": "ad07545d0febd7b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:45.358705Z",
     "start_time": "2025-11-11T15:08:40.731357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "trainer.model = AutoModelForTokenClassification.from_pretrained(\"RyuXiu/xlm-roberta-base-finetuned-panx-de\").to(device)"
   ],
   "id": "9e6b4a268f818af7",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So let s continue, like we saw earlier, you might have a pretty good F1 score, so to make sure our model works as expected,  let‚Äôs try it out on the German version of our simple example.",
   "id": "d995b9d8533454c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:08:45.407815Z",
     "start_time": "2025-11-11T15:08:45.366464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ],
   "id": "d0fe81e4e25acdb2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         0      1      2      3     4     5           6    7     8        9   \\\n",
       "Tokens  <s>  ‚ñÅJeff    ‚ñÅDe     an  ‚ñÅist  ‚ñÅein  ‚ñÅInformati  ker  ‚ñÅbei  ‚ñÅGoogle   \n",
       "Tags      O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n",
       "\n",
       "         10          11     12    13  \n",
       "Tokens  ‚ñÅin  ‚ñÅKaliforni     en  </s>  \n",
       "Tags      O       B-LOC  I-LOC     O  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅJeff</td>\n",
       "      <td>‚ñÅDe</td>\n",
       "      <td>an</td>\n",
       "      <td>‚ñÅist</td>\n",
       "      <td>‚ñÅein</td>\n",
       "      <td>‚ñÅInformati</td>\n",
       "      <td>ker</td>\n",
       "      <td>‚ñÅbei</td>\n",
       "      <td>‚ñÅGoogle</td>\n",
       "      <td>‚ñÅin</td>\n",
       "      <td>‚ñÅKaliforni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It works! But we shouldn‚Äôt get too hyped just because of one good example. What really matters is digging deeper ‚Äî checking where the model messes up and why. That‚Äôs exactly what we‚Äôll do next, for the NER task.\n",
    "\n",
    "Before we jump into the multilingual side of XLM-R, let‚Äôs take a moment to look at where our model went wrong.\n",
    "\n",
    "So let s define a method that we can apply to the validation set:"
   ],
   "id": "a9abaff366e9b0c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:17:58.696982Z",
     "start_time": "2025-11-11T15:17:58.690887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model\n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        # logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "\n",
    "    # Calculate loss per token after flattening batch dimension with view\n",
    "    loss = cross_entropy(output.logits.view(-1, 7),\n",
    "                         labels.view(-1), reduction=\"none\")\n",
    "    # Unflatten batch dimension and convert to numpy array\n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "    return {\"loss\": loss, \"predicted_label\": predicted_label}"
   ],
   "id": "cc73a29e590e948",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now apply this function to the validation set using the `map` method of the dataset and load the results into a Pandas DataFrame for easier analysis.",
   "id": "cd70d61dff543b77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:25:12.425102Z",
     "start_time": "2025-11-11T15:20:05.236826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "valid_set = panx_de_encoded[\"validation\"]\n",
    "valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df = valid_set.to_pandas()"
   ],
   "id": "ee0dc4a2c44024a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:01<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf22c991e22348e48d261fe386b2d1b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The tokens and labels are still stored as IDs, so let‚Äôs map them back to readable strings. For padding tokens labeled as ‚Äì100, we‚Äôll assign a special tag, IGN, so we can filter them out later. We‚Äôll also clean things up by trimming the loss and predicted_label fields to match the actual input length.",
   "id": "9dfe1916316f7a33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:25:31.639596Z",
     "start_time": "2025-11-11T15:25:28.319376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index2tag[-100] = \"IGN\"\n",
    "df[\"input_tokens\"] = df[\"input_ids\"].apply(\n",
    "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df[\"labels\"] = df[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df['loss'] = df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "df['predicted_label'] = df.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\n",
    "df.head(1)"
   ],
   "id": "d466fece1173733",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                            input_ids         attention_mask  \\\n",
       "0  [0, 10699, 11, 15, 16104, 1388, 2]  [1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                        labels  \\\n",
       "0  [IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]   \n",
       "\n",
       "                                                loss  \\\n",
       "0  [0.0, 0.014199061, 0.0, 0.007924309, 0.0074196...   \n",
       "\n",
       "                                     predicted_label  \\\n",
       "0  [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]   \n",
       "\n",
       "                                 input_tokens  \n",
       "0  [<s>, ‚ñÅHam, a, ‚ñÅ(, ‚ñÅUnternehmen, ‚ñÅ), </s>]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]</td>\n",
       "      <td>[0.0, 0.014199061, 0.0, 0.007924309, 0.0074196...</td>\n",
       "      <td>[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[&lt;s&gt;, ‚ñÅHam, a, ‚ñÅ(, ‚ñÅUnternehmen, ‚ñÅ), &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Each column contains a list of tokens, labels, predicted labels, and so on for each\n",
    "sample, Let's take a look at the token individually by unpacking these lists using the `explode` method of the DataFrame.\n",
    "\n",
    "This function allow us to transform each element of a list-like to a row, replicating the index values. Since all the lists in each row have the same length, we can use it.\n",
    "\n",
    "We also filter out the tokens labeled as `IGN` since they correspond to padding tokens or special tokens that we don't care about.\n",
    "\n",
    "Finally, we round the loss values to two decimal places for better readability."
   ],
   "id": "d27e1cfe37e6f929"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:29:37.265149Z",
     "start_time": "2025-11-11T15:29:33.081894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_tokens = df.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "df_tokens.head(7)"
   ],
   "id": "e026d93b3868c84a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  input_ids attention_mask labels  loss predicted_label  input_tokens\n",
       "0     10699              1  B-ORG  0.01           B-ORG          ‚ñÅHam\n",
       "0        15              1  I-ORG  0.01           I-ORG            ‚ñÅ(\n",
       "0     16104              1  I-ORG  0.01           I-ORG  ‚ñÅUnternehmen\n",
       "0      1388              1  I-ORG  0.01           I-ORG            ‚ñÅ)\n",
       "1     56530              1      O  0.00               O           ‚ñÅWE\n",
       "1     83982              1  B-ORG  2.10           B-PER          ‚ñÅLuz\n",
       "1        10              1  I-ORG  2.01           I-PER            ‚ñÅa"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10699</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>‚ñÅHam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>‚ñÅ(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16104</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>‚ñÅUnternehmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1388</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>‚ñÅ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56530</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.00</td>\n",
       "      <td>O</td>\n",
       "      <td>‚ñÅWE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83982</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>2.10</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>‚ñÅLuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>2.01</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>‚ñÅa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that our data‚Äôs in this format, we can group it by input tokens and aggregate their losses ‚Äî counting how often each token appears and calculating its mean and total loss. Then, by sorting everything by total loss, we can see which tokens caused the biggest trouble for our model in the validation set.",
   "id": "3471775c86eb6746"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:37:57.490028Z",
     "start_time": "2025-11-11T15:37:55.328229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(\n",
    "    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ],
   "id": "43217093eef31e89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  0       1       2       3       4     5      6      7  \\\n",
       "input_tokens      ‚ñÅ     ‚ñÅin    ‚ñÅder      ‚ñÅ/    ‚ñÅvon  ‚ñÅund     ‚ñÅ(     ‚ñÅ)   \n",
       "count          6066     989    1388     163     808  1171    246    246   \n",
       "mean           0.03    0.15    0.09    0.66    0.13  0.08   0.31   0.29   \n",
       "sum           196.2  144.77  130.16  106.91  106.02  92.4  76.98  70.68   \n",
       "\n",
       "                  8      9  \n",
       "input_tokens    ‚ñÅ''     ‚ñÅA  \n",
       "count          2898    125  \n",
       "mean           0.02   0.46  \n",
       "sum           65.91  57.14  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>‚ñÅin</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅ/</td>\n",
       "      <td>‚ñÅvon</td>\n",
       "      <td>‚ñÅund</td>\n",
       "      <td>‚ñÅ(</td>\n",
       "      <td>‚ñÅ)</td>\n",
       "      <td>‚ñÅ''</td>\n",
       "      <td>‚ñÅA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6066</td>\n",
       "      <td>989</td>\n",
       "      <td>1388</td>\n",
       "      <td>163</td>\n",
       "      <td>808</td>\n",
       "      <td>1171</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>2898</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>196.2</td>\n",
       "      <td>144.77</td>\n",
       "      <td>130.16</td>\n",
       "      <td>106.91</td>\n",
       "      <td>106.02</td>\n",
       "      <td>92.4</td>\n",
       "      <td>76.98</td>\n",
       "      <td>70.68</td>\n",
       "      <td>65.91</td>\n",
       "      <td>57.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
