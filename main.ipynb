{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Let's get it started with the XTREME benchmark from Hugging Face datasets.\n",
    "To import the dataset, we can use the `load_dataset` function from the `datasets` library.\n",
    "This benchmark includes a variety of tasks across multiple languages, making it a great choice for evaluating multilingual models.\n",
    "It use IOB format for sequence labeling tasks, which is a common format for named entity recognition (NER) and other similar tasks."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:32:46.709396Z",
     "start_time": "2025-09-25T13:32:36.759028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import get_dataset_config_names\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Whoa, that’s a lot of configurations! `XTREME` includes a variety of tasks such as:\n",
    "- Named Entity Recognition (NER)\n",
    "- Part-of-Speech Tagging (POS)\n",
    "- Question Answering (QA)\n",
    "- Sentence Retrieval (SR)\n",
    "\n",
    "But we'll focus on the `NER` task for this example.\n",
    "Let’s narrow the search by just looking for the configurations that start with “`PAN`”\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Because `PAN-X` is the subset of `XTREME` that focuses on `NER` across multiple languages."
   ],
   "id": "d2d4a4d648efcc6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:32:51.547374Z",
     "start_time": "2025-09-25T13:32:51.537192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ],
   "id": "1610d10d121abf3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, we have several configurations for `PAN-X`, each corresponding to a different language.\n",
    "Like you can see, each one has a two-letter language code at the end, such as `en` for English, `de` for German, and `fr` for French. it follows the **ISO 639-1** standard for language codes.\n",
    "\n",
    "Alright, if we want to use the German corpus, we can load it like this:"
   ],
   "id": "7cc81769d30198a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:41:30.327906Z",
     "start_time": "2025-09-25T13:41:16.525392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\")"
   ],
   "id": "3afa6ce53ac69549",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But what if we want to load multiple languages at once? for exemple, Swiss corpus which includes German, French, English and Italian.\n",
    "\n",
    "This corpus is particularly interesting because it reflects the multilingual nature of Switzerland, where multiple languages are spoken and imbalanced.\n",
    "\n",
    "We have like:\n",
    "- 62% of German (de)\n",
    "- 22% of French (fr)\n",
    "- 8% of Italian (it)\n",
    "- 5% of English (en)\n",
    "\n",
    "So, To keep track of each language, let’s create a Python `defaultdict` that stores the language code as the `key` and a `PAN-X` corpus of type DatasetDict as the value:"
   ],
   "id": "5b25a44620f6583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:52:06.590584Z",
     "start_time": "2025-09-25T13:50:46.612413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "# Return a DatasetDict if a key doesn't exist\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # Shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "        ds[split]\n",
    "        .shuffle(seed=0)\n",
    "        .select(range(int(frac * ds[split].num_rows))))\n"
   ],
   "id": "5fb2ff9bd964e1d6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 20000/20000 [00:00<00:00, 938470.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 739892.75 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 857327.64 examples/s]\n",
      "Generating train split: 100%|██████████| 20000/20000 [00:00<00:00, 824190.21 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 386383.06 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 499072.37 examples/s]\n",
      "Generating train split: 100%|██████████| 20000/20000 [00:00<00:00, 599344.68 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 467905.40 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 655564.86 examples/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To ensure that our dataset don't accidentally bias our dataset splits, we `shuffle` each split with a fixed seed before downsampling it according to the spoken proportion.\n",
    "\n",
    "Let's take a look at the number of training examples in each language:"
   ],
   "id": "12e17b36ea46396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T13:59:02.326189Z",
     "start_time": "2025-09-25T13:59:01.877932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    " index=[\"Number of training examples\"])"
   ],
   "id": "a5b4d5be9416d47e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, we have more training examples for Geman than for the other languages, which reflects the linguistic landscape of Switzerland.\n",
    "\n",
    "So, we can use it as a starting point from which zero-shot cross-lingual transfer\n",
    "to French, Italian, and English.\n",
    "\n",
    "Let's take a look at a few examples from the German training set:"
   ],
   "id": "bcd9dd6929c9cadf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:02:27.033231Z",
     "start_time": "2025-09-25T14:02:26.884906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    " print(f\"{key}: {value}\")\n"
   ],
   "id": "eb135a434e2358df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, each example consists of a sentence and its corresponding named entity tags in IOB format.\n",
    "\n",
    "ner_tags column corresponds to the mapping of each entity to a class ID. This is a bit cryptic, so let's add a column that maps each class ID to its corresponding entity label\n",
    "\n",
    "First, let's take a look at the features of the dataset to find the mapping:"
   ],
   "id": "698adffd1c19bbc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:20:54.914357Z",
     "start_time": "2025-09-25T14:20:54.909160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    " print(f\"{key}: {value}\")"
   ],
   "id": "94287dd7473e05d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: List(Value('string'))\n",
      "ner_tags: List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']))\n",
      "langs: List(Value('string'))\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ner_tags` feature is of type `ClassLabel`, which means it has a predefined set of labels.\n",
    "\n",
    "Let's pick up the mapping from class IDs to entity labels:"
   ],
   "id": "95bdf306242efec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:31:08.485484Z",
     "start_time": "2025-09-25T14:31:08.479640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ],
   "id": "8222e2b1bc80c940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ClassLabel` object provides a method called `int2str` that allows us to convert class IDs to their corresponding string labels.\n",
    "With `map` method, we can easily create a new column in the dataset that contains the string labels for each entity tag."
   ],
   "id": "ef8171ffde6599da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:34:25.990679Z",
     "start_time": "2025-09-25T14:34:20.604833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_tag_names(batch):\n",
    " return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ],
   "id": "8e9d82e6e8ed84b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12580/12580 [00:03<00:00, 4122.66 examples/s]\n",
      "Map: 100%|██████████| 6290/6290 [00:01<00:00, 5520.57 examples/s]\n",
      "Map: 100%|██████████| 6290/6290 [00:01<00:00, 5646.22 examples/s]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now, let's take a look at the first example in the German training set with the new `ner_tags_str` column\n",
    "> Yeah, this is a data Analyst Habits! 😅"
   ],
   "id": "c65becd064913364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:36:33.847988Z",
     "start_time": "2025-09-25T14:36:33.744496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "['Tokens', 'Tags'])\n"
   ],
   "id": "95cfcdc3c7f1df5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The presence of the `LOC` tags make sense since the sentence “2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern” means “2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania” in English. And “**Danziger Bucht**” is indeed a location, a bay in the Baltic sea.\n",
   "id": "190867885417e486"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's make a quick check to see if we don't have any unusual imbalance in the tags, let's look at the distribution of each entity across each split.",
   "id": "4f0d19dfec45f92d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T14:47:36.904451Z",
     "start_time": "2025-09-25T14:47:35.306859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ],
   "id": "ed884103a4002a8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a pretty good distribution of entity tags across the `training`, `validation`, and `test` sets.\n",
    "\n",
    "`LOC`, `PER`, and `ORG` are roughly the same for each split, which is what we want to see.\n"
   ],
   "id": "53dbf440fdafb6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
