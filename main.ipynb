{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Let's get it started with the XTREME benchmark from Hugging Face datasets.\n",
    "To import the dataset, we can use the `load_dataset` function from the `datasets` library.\n",
    "This benchmark includes a variety of tasks across multiple languages, making it a great choice for evaluating multilingual models.\n",
    "It use IOB format for sequence labeling tasks, which is a common format for named entity recognition (NER) and other similar tasks."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:51:54.059865Z",
     "start_time": "2025-10-17T13:51:44.703210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Whoa, that‚Äôs a lot of configurations! `XTREME` includes a variety of tasks such as:\n",
    "- Named Entity Recognition (NER)\n",
    "- Part-of-Speech Tagging (POS)\n",
    "- Question Answering (QA)\n",
    "- Sentence Retrieval (SR)\n",
    "\n",
    "But we'll focus on the `NER` task for this example.\n",
    "Let‚Äôs narrow the search by just looking for the configurations that start with ‚Äú`PAN`‚Äù\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Because `PAN-X` is the subset of `XTREME` that focuses on `NER` across multiple languages."
   ],
   "id": "d2d4a4d648efcc6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:31:38.349897Z",
     "start_time": "2025-10-17T12:31:38.330045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ],
   "id": "1610d10d121abf3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, we have several configurations for `PAN-X`, each corresponding to a different language.\n",
    "Like you can see, each one has a two-letter language code at the end, such as `en` for English, `de` for German, and `fr` for French. it follows the **ISO 639-1** standard for language codes.\n",
    "\n",
    "Alright, if we want to use the German corpus, we can load it like this:"
   ],
   "id": "7cc81769d30198a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:31:53.498891Z",
     "start_time": "2025-10-17T12:31:38.861401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset(\"xtreme\", name=\"PAN-X.de\")"
   ],
   "id": "3afa6ce53ac69549",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "But what if we want to load multiple languages at once? for exemple, Swiss corpus which includes German, French, English and Italian.\n",
    "\n",
    "This corpus is particularly interesting because it reflects the multilingual nature of Switzerland, where multiple languages are spoken and imbalanced.\n",
    "\n",
    "We have like:\n",
    "- 62% of German (de)\n",
    "- 22% of French (fr)\n",
    "- 8% of Italian (it)\n",
    "- 5% of English (en)\n",
    "\n",
    "So, To keep track of each language, let‚Äôs create a Python `defaultdict` that stores the language code as the `key` and a `PAN-X` corpus of type DatasetDict as the value:"
   ],
   "id": "5b25a44620f6583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:44.965220Z",
     "start_time": "2025-10-17T12:31:53.522657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "# Return a DatasetDict if a key doesn't exist\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # Shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))\n"
   ],
   "id": "5fb2ff9bd964e1d6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To ensure that our dataset don't accidentally bias our dataset splits, we `shuffle` each split with a fixed seed before downsampling it according to the spoken proportion.\n",
    "\n",
    "Let's take a look at the number of training examples in each language:"
   ],
   "id": "12e17b36ea46396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:45.011838Z",
     "start_time": "2025-10-17T12:32:44.988382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs},\n",
    "             index=[\"Number of training examples\"])"
   ],
   "id": "a5b4d5be9416d47e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                de    fr    it    en\n",
       "Number of training examples  12580  4580  1680  1180"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, we have more training examples for Geman than for the other languages, which reflects the linguistic landscape of Switzerland.\n",
    "\n",
    "So, we can use it as a starting point from which zero-shot cross-lingual transfer\n",
    "to French, Italian, and English.\n",
    "\n",
    "Let's take a look at a few examples from the German training set:"
   ],
   "id": "bcd9dd6929c9cadf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:45.238048Z",
     "start_time": "2025-10-17T12:32:45.201017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ],
   "id": "eb135a434e2358df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, each example consists of a sentence and its corresponding named entity tags in IOB format.\n",
    "\n",
    "ner_tags column corresponds to the mapping of each entity to a class ID. This is a bit cryptic, so let's add a column that maps each class ID to its corresponding entity label\n",
    "\n",
    "First, let's take a look at the features of the dataset to find the mapping:"
   ],
   "id": "698adffd1c19bbc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:45.512631Z",
     "start_time": "2025-10-17T12:32:45.500036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "id": "94287dd7473e05d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: List(Value('string'))\n",
      "ner_tags: List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']))\n",
      "langs: List(Value('string'))\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ner_tags` feature is of type `ClassLabel`, which means it has a predefined set of labels.\n",
    "\n",
    "Let's pick up the mapping from class IDs to entity labels:"
   ],
   "id": "95bdf306242efec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:45.734657Z",
     "start_time": "2025-10-17T12:32:45.718173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ],
   "id": "8222e2b1bc80c940",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ClassLabel` object provides a method called `int2str` that allows us to convert class IDs to their corresponding string labels.\n",
    "With `map` method, we can easily create a new column in the dataset that contains the string labels for each entity tag."
   ],
   "id": "ef8171ffde6599da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:46.000468Z",
     "start_time": "2025-10-17T12:32:45.911997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "\n",
    "\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)"
   ],
   "id": "8e9d82e6e8ed84b6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And now, let's take a look at the first example in the German training set with the new `ner_tags_str` column\n",
    "> Yeah, this is a data Analyst Habits! üòÖ"
   ],
   "id": "c65becd064913364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:46.174223Z",
     "start_time": "2025-10-17T12:32:46.023452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
    "             ['Tokens', 'Tags'])\n"
   ],
   "id": "95cfcdc3c7f1df5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The presence of the `LOC` tags make sense since the sentence ‚Äú2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern‚Äù means ‚Äú2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania‚Äù in English. And ‚Äú**Danziger Bucht**‚Äù is indeed a location, a bay in the Baltic sea.\n",
   "id": "190867885417e486"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's make a quick check to see if we don't have any unusual imbalance in the tags, let's look at the distribution of each entity across each split.",
   "id": "4f0d19dfec45f92d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:32:48.390597Z",
     "start_time": "2025-10-17T12:32:46.571031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ],
   "id": "ed884103a4002a8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a pretty good distribution of entity tags across the `training`, `validation`, and `test` sets.\n",
    "\n",
    "`LOC`, `PER`, and `ORG` are roughly the same for each split, which is what we want to see.\n"
   ],
   "id": "53dbf440fdafb6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# What are we going to do next?\n",
    "\n",
    "Like i mentioned earlier, we are going to make a zero-shot cross-lingual transfer from German to French, Italian, and English.\n",
    "\n",
    "What is Zero-shot cross-lingual transfer?\n",
    "\n",
    ">In short, it means training a model on one language (German in this case) and then evaluating its performance on other languages (French, Italian, and English) without any additional training on those languages."
   ],
   "id": "d4960d895eed2258"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So, now, we need a model to evaluate.\n",
    "\n",
    "One of the first multilingual transformers was `mBERT`, which uses the same architecture and pretraining objective as `BERT` but is trained on additional data from wikipedia articles in many languages. After that, there are many other models like `XLM-R`, `mT5`, and `mDeBERTaV3` that have shown even better performance on various multilingual benchmarks.\n",
    "\n",
    "So we will focus on `XLM-R`, which is a robustly optimized version of `mBERT` and has shown strong performance on various multilingual benchmarks, including `XTREME`."
   ],
   "id": "3e0ed00c1e6685e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`XLM-R` is a transformer-based model like `BERT`, but use a `SentencePiece` tokenizer instead of `WordPiece` tokenizer used in `BERT`.\n",
    "\n",
    "To get a feel for how SentencePiece compares to WordPiece, let's load the BERT tokenizer and the XLM-R tokenizer in ü§ó(Hugging Face) Transformers."
   ],
   "id": "8a243c140dd01f19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:20.454658Z",
     "start_time": "2025-10-17T12:32:48.481475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ],
   "id": "56855a88b019c457",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now, let's take a example sentence",
   "id": "738d9ebfce23507b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:21.039377Z",
     "start_time": "2025-10-17T12:33:20.801865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "bert_tokens, xlmr_tokens"
   ],
   "id": "53ba2d8cdef9d727",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]'],\n",
       " ['<s>', '‚ñÅJack', '‚ñÅSpar', 'row', '‚ñÅlove', 's', '‚ñÅNew', '‚ñÅYork', '!', '</s>'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, one of the main differences is that instead of the `[CLS]` and `[SEP]` tokens that `BERT` uses for sentence\n",
    "classification tasks, `XLM-R` uses `<s>` and `<\\s>` to denote the start and end of a sequence\n",
    "\n",
    "One other difference is how the two tokenizers handle subword tokenization. `BERT` uses `##` to indicate that a token is a continuation of the previous token, while `XLM-R` uses `‚ñÅ` to indicate the start of a new word or just a space.\n",
    "\n",
    "Here, `BERT` tokenizer lost the information that there is no whitespace between ‚ÄúYork‚Äù and ‚Äú!‚Äù\n",
    "\n",
    "Why?\n",
    "\n",
    ">In short, the `WordPiece` tokenizer first splits text on whitespace and punctuation, then breaks each word into subword units from its vocabulary. It does not explicitly preserve whitespace information.\n",
    "In contrast, the `SentencePiece` tokenizer treats the input as a raw character sequence and uses a statistical model (`Unigram` or `BPE`) to segment it, encoding spaces explicitly and requiring no pre-tokenization."
   ],
   "id": "b00495ba73b1fd3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:21.327968Z",
     "start_time": "2025-10-17T12:33:21.306529Z"
    }
   },
   "cell_type": "code",
   "source": "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")",
   "id": "537ba10cbc0d96eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Do you know how is the transformer model architecture looks like?\n",
    "in general, you have two main components:\n",
    "1. The model body (the transformer layers)\n",
    "2. The task-specific head (like a classification head for text classification tasks or a token classification head for NER tasks)\n",
    "\n",
    "so, take it like a sample exercise, let's implement a token classification model using `XLM-R` as the model body.\n",
    "\n",
    "> Note: In fact, ü§ó Transformers library already provides a pre-implemented class called `XLMRobertaForTokenClassification` that does exactly this. But implementing it from scratch is a great way to understand how these models work under the hood. And if someday you need a custom modification, you will know where to start.\n"
   ],
   "id": "f22d5818b3c233fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:22.899406Z",
     "start_time": "2025-10-17T12:33:21.586962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)\n"
   ],
   "id": "d8d550f1198eda96",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, we first load the `XLM-R` model body using the `RobertaModel` class from ü§ó Transformers. Then, we set up a token classification head consisting of a dropout layer followed by a linear layer that maps the hidden states to the number of labels.\n",
    "\n",
    "My custom class inherits from `RobertaPreTrainedModel`, which provides useful methods for loading and saving pretrained models.\n",
    "\n",
    "But, you might wondering, what does do `config_class` ?\n",
    "> In ü§ó transformers, each model inherits from a base class like `PreTrainedModel` (here `RobertaPreTrainedModel`).\n",
    "> These base classes define several utility methods such as:\n",
    "> - rom_pretrained(...)\n",
    "> - save_pretrained(...)\n",
    "> - from_config(...)\n",
    ">\n",
    "> These methods often need to know which configuration class to use for the specific model.\n",
    "> By setting the `config_class` attribute, we inform the base class about the appropriate configuration class to use when instantiating the model from a configuration object.\n",
    "\n",
    "Note that, we set `add_pooling_layer=False` to ensure all `hidden states` are returned and not only the one\n",
    "associated with the `[CLS]` token"
   ],
   "id": "5dd58d041fbd9ac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we are ready to load our token classification model. However, first, We‚Äôll need to provide some\n",
    "additional information beyond the model name, including the tags that we will use to\n",
    "label each entity and the mapping of each tag to an ID and vice versa"
   ],
   "id": "fe1b2f1f99ed08e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:23.071266Z",
     "start_time": "2025-10-17T12:33:23.061250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ],
   "id": "af146f68ad96f621",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll store thes mapping and the `tags.num_classes` attribute in the AutoConfig object by passing them as keyword arguments to the `from_pretrained` method.",
   "id": "ddb9f3a0ff5fcf8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:23.612203Z",
     "start_time": "2025-10-17T12:33:23.248675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ],
   "id": "a86af49f17eae502",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can load our token classification model using the `from_pretrained` method of our custom class and passing the configuration object we just created.",
   "id": "f44d31257c10314e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:51:38.649522Z",
     "start_time": "2025-10-17T13:51:38.146620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ],
   "id": "ccf323552ca0088f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's make a quick check to see if the model is working as expected",
   "id": "b59c1cda2e0ba41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:26.440592Z",
     "start_time": "2025-10-17T12:33:26.337169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ],
   "id": "101a353e004d9569",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ‚ñÅJack  ‚ñÅSpar    row  ‚ñÅlove  s  ‚ñÅNew  ‚ñÅYork   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅJack</td>\n",
       "      <td>‚ñÅSpar</td>\n",
       "      <td>row</td>\n",
       "      <td>‚ñÅlove</td>\n",
       "      <td>s</td>\n",
       "      <td>‚ñÅNew</td>\n",
       "      <td>‚ñÅYork</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we pass the input IDs to the model and extract the predicted class for each token by taking the `argmax` of the output logits.",
   "id": "4defe6f1bba388a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:31.396743Z",
     "start_time": "2025-10-17T12:33:26.867407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ],
   "id": "2e4150f965fd19f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n",
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like you can see, the logits have the shape `[batch_size, num_tokens, num_tags]`, with each token having a score for each seven possible tags.\n",
    "\n",
    "By enumerating over the sequence, we can quickly see the predicted tag for each token."
   ],
   "id": "6eba1e6979fa43aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:32.193743Z",
     "start_time": "2025-10-17T12:33:32.154102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ],
   "id": "1c4547c9a36f4816",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ‚ñÅJack  ‚ñÅSpar    row  ‚ñÅlove      s   ‚ñÅNew  ‚ñÅYork      !   </s>\n",
       "Tags    I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅJack</td>\n",
       "      <td>‚ñÅSpar</td>\n",
       "      <td>row</td>\n",
       "      <td>‚ñÅlove</td>\n",
       "      <td>s</td>\n",
       "      <td>‚ñÅNew</td>\n",
       "      <td>‚ñÅYork</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So... I don't think \"Jack\" is a Location, right? üòÖ\n",
    "\n",
    "But well, with random weights ‚Äî what did I expect anyway üòÖ\n",
    "\n",
    "Okay, time to fine-tune it on some labeled data ‚Äî let‚Äôs make it smarter üòé\n",
    "Before that, though, let‚Äôs wrap the previous steps into a neat helper function."
   ],
   "id": "c7f949982da5081f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:33:32.587675Z",
     "start_time": "2025-10-17T12:33:32.582175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model(input_ids)[0]\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ],
   "id": "45cc5cbc5df2361a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before we start to train our model, we need to tokenize the input and prepare the labels.\n",
    "\n",
    "So, like we can see, the tokenizer and model can encode a single example. Our next step is to tokenize the entire dataset so that we can feed it into the model for fine-tuning.\n",
    "\n",
    "ü§ó Datasets provide a convenient `map` method that allows us to apply a function to each example in the dataset.\n",
    "\n",
    "For that, we need to define a function with the minimum signature :\n",
    "\n",
    "`function(examples: Dict[str, List]) -> Dict[str, List]`\n",
    "\n",
    "Since the XLM-R tokenizer returns the input IDs, but we also need :\n",
    "- first the attention masks to indicate which tokens are real tokens and which are padding tokens.\n",
    "- second, the label IDs to say which tag (e.g., `B-PER`, `I-LOC`, etc.) corresponds to each token.\n",
    "\n",
    "Let's look at how this works for a single example:"
   ],
   "id": "517cb98c06c0c04f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:08:15.444440Z",
     "start_time": "2025-10-17T13:08:15.433585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n",
    "words, labels"
   ],
   "id": "6d148ca283adb3a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2.000',\n",
       "  'Einwohnern',\n",
       "  'an',\n",
       "  'der',\n",
       "  'Danziger',\n",
       "  'Bucht',\n",
       "  'in',\n",
       "  'der',\n",
       "  'polnischen',\n",
       "  'Woiwodschaft',\n",
       "  'Pommern',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we can use the tokenizer to tokenize the input words. Since our input is already tokenized into words, we need to set the `is_split_into_words` parameter to `True` to say to the tokenizer that the input is already split into words.",
   "id": "dd42ed9376fbcdc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:50:50.815293Z",
     "start_time": "2025-10-17T12:50:50.691208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ],
   "id": "e961cc2f6912ffdd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         0       1           2  3    4     5     6   7    8      9   ...   15  \\\n",
       "Tokens  <s>  ‚ñÅ2.000  ‚ñÅEinwohner  n  ‚ñÅan  ‚ñÅder  ‚ñÅDan  zi  ger  ‚ñÅBuch  ...  ‚ñÅWo   \n",
       "\n",
       "       16   17      18   19    20 21 22 23    24  \n",
       "Tokens  i  wod  schaft  ‚ñÅPo  mmer  n  ‚ñÅ  .  </s>  \n",
       "\n",
       "[1 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅ2.000</td>\n",
       "      <td>‚ñÅEinwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅan</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅDan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>‚ñÅBuch</td>\n",
       "      <td>...</td>\n",
       "      <td>‚ñÅWo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>‚ñÅPo</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Like we can see, the tokenizer has split \"_Einwohnern\" into two subwords: \"‚ñÅEinwohner\" and \"n\".\n",
    "\n",
    "Since we're following the convention that \"_Einwohner\" should be associated with the \"B-LOC\", we need a way to mask the subword representations after the first one. Fortunately, the tokenizer provides a method called `word_ids()` that returns a list mapping each token to its corresponding word index in the original input."
   ],
   "id": "783ce7254a427133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:07:33.884922Z",
     "start_time": "2025-10-17T13:07:33.857096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ],
   "id": "49fbe25e588f2485",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0       1           2  3    4     5     6   7    8      9   ...  \\\n",
       "Tokens     <s>  ‚ñÅ2.000  ‚ñÅEinwohner  n  ‚ñÅan  ‚ñÅder  ‚ñÅDan  zi  ger  ‚ñÅBuch  ...   \n",
       "Word IDs  None       0           1  1    2     3     4   4    4      5  ...   \n",
       "\n",
       "           15 16   17      18   19    20  21  22  23    24  \n",
       "Tokens    ‚ñÅWo  i  wod  schaft  ‚ñÅPo  mmer   n   ‚ñÅ   .  </s>  \n",
       "Word IDs    9  9    9       9   10    10  10  11  11  None  \n",
       "\n",
       "[2 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅ2.000</td>\n",
       "      <td>‚ñÅEinwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅan</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅDan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>‚ñÅBuch</td>\n",
       "      <td>...</td>\n",
       "      <td>‚ñÅWo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>‚ñÅPo</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, each token is associated with a word index, where `None` indicates special tokens like `<s>` and `<\\s>`.\n",
    "The original words are indexed from `0` to `n-1`, where `n` is the number of words in the input.\n",
    "\n",
    "So... we can use this mapping to align the original labels with the tokenized input and now we know if for exemple \"n\" is a continuation of the previous word or not.\n",
    "\n",
    "Let's set `-100` for the tokens that we want to ignore during loss computation."
   ],
   "id": "daa8148829d29a76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:22:38.269591Z",
     "start_time": "2025-10-17T13:22:38.242845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ],
   "id": "8f4b8fc5c4b20a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             0       1           2     3    4     5      6     7     8   \\\n",
       "Tokens      <s>  ‚ñÅ2.000  ‚ñÅEinwohner     n  ‚ñÅan  ‚ñÅder   ‚ñÅDan    zi   ger   \n",
       "Word IDs   None       0           1     1    2     3      4     4     4   \n",
       "Label IDs  -100       0           0  -100    0     0      5  -100  -100   \n",
       "Labels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n",
       "\n",
       "              9   ...     15    16    17      18     19    20    21  22    23  \\\n",
       "Tokens     ‚ñÅBuch  ...    ‚ñÅWo     i   wod  schaft    ‚ñÅPo  mmer     n   ‚ñÅ     .   \n",
       "Word IDs       5  ...      9     9     9       9     10    10    10  11    11   \n",
       "Label IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \n",
       "Labels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n",
       "\n",
       "             24  \n",
       "Tokens     </s>  \n",
       "Word IDs   None  \n",
       "Label IDs  -100  \n",
       "Labels      IGN  \n",
       "\n",
       "[4 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅ2.000</td>\n",
       "      <td>‚ñÅEinwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅan</td>\n",
       "      <td>‚ñÅder</td>\n",
       "      <td>‚ñÅDan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>‚ñÅBuch</td>\n",
       "      <td>...</td>\n",
       "      <td>‚ñÅWo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>‚ñÅPo</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>‚ñÅ</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>...</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Maybe you might wonder, **why ignore the special tokens and subword tokens during loss computation?**\n",
    "\n",
    "> Ignoring special tokens and subword tokens during loss computation is important because these tokens do not correspond to actual words in the input text. Including them in the loss calculation could introduce noise and lead to incorrect learning signals for the model.\n",
    "\n",
    "And second, **why set the label of subword tokens to -100 specifically?**\n",
    "\n",
    "> In PyTorch's `CrossEntropyLoss`, the label `-100` is used as a special value to indicate that a particular token should be ignored during loss computation. This is a convention in PyTorch, and using `-100` allows us to effectively exclude those tokens from contributing to the loss, ensuring that the model focuses on learning from the relevant tokens only.\n",
    "\n",
    "Now, we can wrap all these steps into a single function that we can use with the `map` method of the dataset."
   ],
   "id": "97ed2be56cdef43f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:43:35.168762Z",
     "start_time": "2025-10-17T13:43:35.161274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ],
   "id": "e85596aff2f7a096",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can write a function to iterate over.",
   "id": "d1b924063af41c02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:45:37.614852Z",
     "start_time": "2025-10-17T13:45:37.610701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])"
   ],
   "id": "ed540346c1f1e285",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When we use it to a DatasetDict, we'll get a encoded DatasetDict per split.",
   "id": "62c3207ce238c9a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:51:35.687985Z",
     "start_time": "2025-10-17T13:51:19.464284Z"
    }
   },
   "cell_type": "code",
   "source": "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])",
   "id": "c1a2d4f1854ec745",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12580/12580 [00:12<00:00, 1002.17 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6290/6290 [00:01<00:00, 4757.25 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6290/6290 [00:01<00:00, 6188.71 examples/s]\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
